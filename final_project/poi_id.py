#!/usr/bin/pythonimport sysimport picklesys.path.append("../tools/")from feature_format import featureFormat, targetFeatureSplitfrom tester import dump_classifier_and_dataimport pandasimport numpy as npfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classiffrom sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegressionfrom sklearn.decomposition import PCAfrom sklearn.ensemble import ExtraTreesClassifierfrom sklearn.model_selection import StratifiedShuffleSplitimport matplotlib.pyplot as pltfrom sklearn.svm import SVCfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.feature_selection import RFECVimport matplotlib.pyplotfrom sklearn.metrics import accuracy_scorefrom sklearn.metrics import precision_scorefrom sklearn.model_selection import cross_val_score#from sklearn.datasets import make_classification#=================================================================================#### Task 1: Select what features you'll use.### features_list is a list of strings, each of which is a feature name.### The first feature must be "poi".### for feature selection we run the selectKbest on the data set.enron_data = pickle.load(open("../final_project/final_project_dataset.pkl", "r")) #names of the columns in the 'enron_pickle_features.csv' data set.names = ["salary" ,  "to_messages" , "deferral_payments" , "total_payments" ,        "exercised_stock_options",   "bonus" ,  "restricted_stock" , "shared_receipt_with_poi" ,  "restricted_stock_deferred", "total_stock_value",    "expenses"  , "loan_advances" ,        "from_messages" ,  "other" , "from_this_person_to_poi",       "director_fees"  ,  "deferred_income" , "long_term_incentive",            "from_poi_to_this_person"]#getting the data and then reading them into dataframes for manipulation and feature#selectiondef makeDataFramesFeature():    enron_dataframe = pandas.read_csv('enron_pickle_features.csv')    enron_poi = pandas.read_csv('enron_poi.csv')    del enron_dataframe['Unnamed: 0']    del enron_poi['Unnamed: 0']    array_data = enron_dataframe.values    X = array_data[:,0:19]    array_poi = enron_poi.values.ravel()    Y = array_poi    return  X, Y#X, Y = makeDataFramesFeature()def featureSelectKbest(X,Y): #Cannot be used since we have negative values    test = SelectKBest(score_func= chi2, k=4)    fit = test.fit(X, Y)    # summarize scores    np.set_printoptions(precision=3)    print(fit.scores_)    features = fit.transform(X)    # summarize selected features    print(features[0:6,:])    def featureRFE(X,Y): #recursive feature elimination using a random forest cl    from sklearn.ensemble import RandomForestClassifier    model = RandomForestClassifier(n_estimators=200)    rfe = RFE(model, 4)    fit = rfe.fit(X, Y)    print("Num Features: %d") % fit.n_features_    print("Selected Features: %s") % fit.support_    print("Feature Ranking: %s") % fit.ranking_    #print names    i = 0    namesdict = {}    for values in names:        namesdict[values] = fit.support_[i]        i +=1    print namesdict#==============================================================================# featureRFEout = { 'salary': False, 'to_messages': False, \# 'deferral_payments': False, 'total_payments': False, 'loan_advances': False, \# 'bonus': True, 'restricted_stock_deferred': False, 'total_stock_value': True, \# 'shared_receipt_with_poi': False, 'long_term_incentive': False, \# 'exercised_stock_options': False, 'from_messages': False, 'other': True, \# 'from_poi_to_this_person': False, 'from_this_person_to_poi': False, \# 'deferred_income': False, 'expenses': True, 'restricted_stock': False, \# 'director_fees': False }#==============================================================================    #Recursive feature elimination using random forests suggest I should use#Bonus, total_stock_value, expenses and other (we can ignore this for now)def featureRFECross(X,Y): #recursive feature elimination super nice methoed to automatically#select good feature using a cross validation for classification    svc = SVC(kernel="rbf") #taking too much time.    #model = LogisticRegression()    # The "accuracy" scoring is proportional to the number of correct    # classifications    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),              scoring='accuracy')    rfecv.fit(X, Y)    print("Optimal number of features : %d" % rfecv.n_features_)# Plot number of features VS. cross-validation scores    plt.figure()    plt.xlabel("Number of features selected")    plt.ylabel("Cross validation score (nb of correct classifications)")    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)    plt.show()    i = 0    namesdict = {}    for values in names:        namesdict[values] = rfecv.support_[i]        i +=1    print namesdict    def featurePCA(X,Y): #also tried this but I could not figure out how to name the features    # for feature extraction feature extraction    pca = PCA(n_components=4)    fit = pca.fit(X)    # summarize components    print("Explained Variance: %s") % fit.explained_variance_ratio_    print(fit.components_)    def featureETC(X,Y): #extra method learnt to select features    # feature extraction using bagged trees    namesdict = {}    model = ExtraTreesClassifier()    model.fit(X, Y)    #print(model.feature_importances_)    #print names    i = 0    for values in names:        namesdict[values] = model.feature_importances_[i]        i +=1    print namesdict              #==============================================================================# featureETCoutput = {'salary': 0.041632257710185139, 'to_messages': 0.039859107742246484,\#                     'deferral_payments': 0.024074150367185947, \#                     'total_payments': 0.064555518959854619, \#                     'loan_advances': 0.0093116015112734048, \#                     'bonus': 0.073052292449863596, \#                     'restricted_stock_deferred': 0.0042245370370370353,\#                     'total_stock_value': 0.09369028839725424, \#                     'shared_receipt_with_poi': 0.071123487226284629, \#                     'long_term_incentive': 0.04046480320342271, \#                     'exercised_stock_options': 0.1129150917901071, \#                     'from_messages': 0.024095798427395669, \#                     'other': 0.054538316103403574, \#                     'from_poi_to_this_person': 0.036628178381556319, \#                     'from_this_person_to_poi': 0.052257500165677206, \#                     'deferred_income': 0.10772890822303453, \#                     'expenses': 0.10845624961908869,\#                     'restricted_stock': 0.038109225603169372, \#                     'director_fees': 0.0032826870819597218}#==============================================================================    #the results from feature  RFE selection recommended using , exercised stock value, total_stock_value, expenses  #===================================================================================#features_list = ['poi','salary', 'total_stock_value', 'expenses', 'bonus'] # You will need to use more features + your feature### Load the dictionary containing the dataset with open("final_project_dataset.pkl", "r") as data_file:     data_dict = pickle.load(data_file)#===================================================================================###plotting for outlier removal### Task 2: Remove outliers #done in .rmd filedata1 = featureFormat(data_dict, features_list)def plotSalaryStock(data):    for point in data :        salary = point[1]        total_stock_value = point[2]        matplotlib.pyplot.scatter(salary, total_stock_value)    matplotlib.pyplot.xlabel("salary")    matplotlib.pyplot.ylabel("total_stock_value")    matplotlib.pyplot.show()def plotSalaryExpenses(data):    for point in data :        salary = point[1]        expenses = point[3]        matplotlib.pyplot.scatter(salary, expenses)        matplotlib.pyplot.xlabel("salary")    matplotlib.pyplot.ylabel("expenses")    matplotlib.pyplot.show()def plotStockExpenses(data):    for point in data :        total_stock_value = point[2]        expenses = point[3]        matplotlib.pyplot.scatter(total_stock_value, expenses)        matplotlib.pyplot.xlabel("total_stock_value")    matplotlib.pyplot.ylabel("expenses")    matplotlib.pyplot.show()def plotSalaryBonus(data):    for point in data :        salary = point[1]        bonus = point[4]        matplotlib.pyplot.scatter(salary, bonus)        matplotlib.pyplot.xlabel("salary")    matplotlib.pyplot.ylabel("bonus")    matplotlib.pyplot.show()            plotSalaryStock(data1)plotSalaryExpenses(data1)plotStockExpenses(data1)plotSalaryBonus(data1)#outliers were noticed in data in salary, bonus and total stock value#===================================================================================##outlier removal functionsdef salaryOut():    outlierlist = []    for keys in data_dict :        if type(data_dict[keys]["salary"]) == int or keys ==  "TRAVEL AGENCY IN THE PARK" or keys == "LOCKHART EUGENE E":            if data_dict[keys]["salary"] > 2.5e7:                data_dict[keys]["salary"] = "NaN"                outlierlist.append(keys)            elif data_dict[keys]["salary"] < 0:                data_dict[keys]["salary"] = "NaN"                outlierlist.append(keys)            else:                pass                  else:            pass    print "Salary outliers removed :", outlierlist, "\n"    def totalStockValueOut():    outlierlist = []    for keys in data_dict :        if type(data_dict[keys]["total_stock_value"]) == int or keys ==  "TRAVEL AGENCY IN THE PARK" or keys == "LOCKHART EUGENE E" :            if data_dict[keys]["total_stock_value"] > 4.0e8:                data_dict[keys]["total_stock_value"] = "NaN"                outlierlist.append(keys)            elif data_dict[keys]["total_stock_value"] < 0:                data_dict[keys]["total_stock_value"] = "NaN"                outlierlist.append(keys)            else:                pass        else:            pass    print "Total stock value outliers removed :",outlierlist, "\n"    def bonusOut():    outlierlist = []    for keys in data_dict :        if type(data_dict[keys]["bonus"]) == int or keys ==  "TRAVEL AGENCY IN THE PARK" or keys == "LOCKHART EUGENE E":            if data_dict[keys]["bonus"] > 0.8e8:                data_dict[keys]["bonus"] = "NaN"                outlierlist.append(keys)            elif data_dict[keys]["bonus"]< 0:                data_dict[keys]["bonus"] = "NaN"                outlierlist.append(keys)            else:                pass                 else:            pass    print "Bonus outliers removed :" , outlierlist, "\n"def expenseOut():    outlierlist = []    for keys in data_dict :        if type(data_dict[keys]["expenses"]) == int or keys ==  "TRAVEL AGENCY IN THE PARK" or keys == "LOCKHART EUGENE E":            if data_dict[keys]["expenses"] > 5.0e6:                data_dict[keys]["expenses"] = "NaN"                outlierlist.append(keys)            elif data_dict[keys]["expenses"]< 0:                data_dict[keys]["expenses"] = "NaN"                outlierlist.append(keys)            else:                pass                     else:            pass    print "Expenses outliers removed ", outlierlist, "\n"#removing salary outliers#removing total_stock_value outliers#removing bonus outliers#removing expense outlierssalaryOut()totalStockValueOut()bonusOut()expenseOut()#===================================================================================##replotting removed outliersprint "Take a look at outlier removed graphs :"#reassinging data to new mat for looking at plots from new modified dictionairesdata2 = featureFormat(data_dict, features_list)#plotting to see the effect of having removed outliersplotSalaryStock(data2)plotSalaryExpenses(data2)plotStockExpenses(data2)plotSalaryBonus(data2)    ### Task 3: Create new feature(s) #===================================================================================##making the new feature#Total number of "to emails" to this person - shared_receipt_with_poi - sent_by_poi_to_this_person.#The lower this number greater the relative importance since it means that most \#of the communication to this person involved conversation with a poi or in the \#same conversation with a poi. The number of emails from other people to this \#person was low.  def newFeature():    for keys in data_dict :        if data_dict[keys].get("to_messages") and data_dict[keys].get("shared_receipt_with_poi") and data_dict[keys].get("from_poi_to_this_person") != 'NaN' :            to = data_dict[keys].get("to_messages")             shared = data_dict[keys].get("shared_receipt_with_poi")             from_poi = data_dict[keys].get("from_poi_to_this_person")            data_dict[keys]["relative_importance"] = to - (shared + from_poi)        else:            data_dict[keys]["relative_importance"] = "NaN"                    #print "relative_importance :",  data_dict[keys].get("relative_importance")                #print enron_data[keys].get("relative_importance")newFeature()def extractRelativeImportance(): #funtion to see the values of relative importance    i = []    j = []    for keys in data_dict:        if data_dict[keys]["relative_importance"] != 'NaN':            i.append(data_dict[keys].get("relative_importance"))            j.append(keys)        else:            pass    return i, j        ### Store to my_dataset for easy export below.#enron_dataframe.insert(my_dataset = data_dict#including the new features in the feature listfeatures_list2 = ['poi','salary', 'total_stock_value', 'expenses', 'bonus',  'relative_importance'] ### Extract features and labels from dataset for local testingdata3 = featureFormat(my_dataset, features_list2, sort_keys = True)labels, features = targetFeatureSplit(data3)## done for finding out number of features to use.features_list4 = ["poi", "salary" ,  "to_messages" , "deferral_payments" , "total_payments" ,        "exercised_stock_options",   "bonus" ,  "restricted_stock" , "shared_receipt_with_poi" ,  "restricted_stock_deferred", "total_stock_value",    "expenses"  , "loan_advances" ,        "from_messages" ,  "other" , "from_this_person_to_poi",       "director_fees"  ,  "deferred_income" , "long_term_incentive",            "from_poi_to_this_person"]  data4 = featureFormat(my_dataset, features_list4, sort_keys = True)labels4, features4 = targetFeatureSplit(data4)labels4 = np.array(labels4)features4 = np.array(features4)#===================================================================================##trying the classifiers ### Task 4: Try a varity of classifiers### Please name your classifier clf for easy export below.### Note that if you want to do PCA or other multi-stage operations,### you'll need to use Pipelines. For more info:### http://scikit-learn.org/stable/modules/pipeline.html# Provided to give you a starting point. Try a variety of classifiers.def classifyNB():       from sklearn.naive_bayes import GaussianNB    clf = GaussianNB()    clf.fit(features_train, labels_train)    pred = clf.predict(features_test)    acc = accuracy_score(labels_test, pred)    pres = precision_score(labels_test, pred)    print acc , "NB Accuracy" , pres, "NB precision","\n"    return clf , acc, pres    def classifyDT():    from sklearn import tree    clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', \                                      max_depth=None, min_samples_split=2, \                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\                                      max_features=None, random_state=8, \                                      max_leaf_nodes=None, min_impurity_split=1e-07,\                                      class_weight=None, presort=False)    clf.fit(features_train, labels_train)    pred = clf.predict(features_test)    acc = accuracy_score(labels_test, pred)    pres = precision_score(labels_test, pred)    print acc , "DT Accuracy" , pres, "DT precision", "\n"    return clf , acc, pres    def classifySVM():    from sklearn.svm import SVC    clf = SVC(C=0.9, cache_size=200, class_weight=None, coef0=0.0,    decision_function_shape=None, degree=3, gamma='auto', kernel='poly')    clf.fit(features_train, labels_train)    pred = clf.predict(features_test)    acc = accuracy_score(labels_test, pred)    pres = precision_score(labels_test, pred)    print acc , "SVM Accuracy" , pres, "SVM precision"    , "\n"    return clf , acc, pres    def classifyRF():    from sklearn.ensemble import RandomForestClassifier    clf = RandomForestClassifier(n_estimators=200)    clf.fit(features_train, labels_train)    pred = clf.predict(features_test)    acc = accuracy_score(labels_test, pred)    pres = precision_score(labels_test, pred)    print acc , "RF Accuracy" , pres, "RF precision" , "\n"    return clf , acc, pres    def classifyAB():    from sklearn.ensemble import AdaBoostClassifier        clf = AdaBoostClassifier(base_estimator=None, n_estimators=200, \                             learning_rate=0.5, algorithm='SAMME', random_state=None)    clf.fit(features_train, labels_train)    pred = clf.predict(features_test)    acc = accuracy_score(labels_test, pred)    pres = precision_score(labels_test, pred)    print acc , "AB Accuracy" , pres, "AB precision"          scores = cross_val_score(clf,features_test,pred )    print scores.mean() , "AB Cross val score" , "\n"    return clf , acc, pres    #give the number of splits on the validation step    n_splits = 5    SSS = StratifiedShuffleSplit(n_splits, test_size=0.5, random_state=0)SSS.get_n_splits(features, labels)print SSS ,"\n"print "performing a stratified shuffle split for the validation process \to ensure that an equal ratio of POIs to non-POIs are present in the training and \test set", "\n"accuracyNB = []precisionNB = []accuracyRF = []precisionRF = []accuracyAB = []precisionAB = []accuracyDT = []precisionDT = []    for train_index, test_index in SSS.split(features, labels):   print "=================================================================="    print "TRAIN:", train_index, "\n" "TEST:", test_index, "\n"   features = np.array(features)   labels = np.array(labels)   features_train, features_test = features[train_index], features[test_index]   labels_train, labels_test = labels[train_index], labels[test_index]             clf1, acc1, prec1 = classifyNB()   clf2, acc2, prec2 = classifyRF()   clf3, acc3, prec3 = classifyAB()   clf4, acc4, prec4 = classifyDT()      accuracyNB.append(acc1)   precisionNB.append(prec1)   accuracyRF.append(acc2)   precisionRF.append(prec2)   accuracyAB.append(acc3)   precisionAB.append(prec3)   accuracyDT.append(acc4)   precisionDT.append(prec4)      print "=================================================================="print "average accuracy NB : " , float(sum(accuracyNB))/n_splits  print "precision accuracy NB : " , float(sum(precisionNB))/n_splits print "average accuracy RF : " , float(sum(accuracyRF))/n_splits  print "precision accuracy RF : " , float(sum(precisionRF))/n_splits  print "average accuracy AB : " , float(sum(accuracyAB))/n_splits  print "precision accuracy AB : " , float(sum(precisionAB))/n_splits  print "average accuracy DT : " , float(sum(accuracyDT))/n_splits  print "precision accuracy DT : " , float(sum(precisionDT))/n_splits     #===================================================================================##exporting featuresmy_dataset = my_datasetfeatures_list = features_listclf, acc, prec = classifyDT()#clf = classifyAB()#clf = classifySVM() #clf = classifyDT()dump_classifier_and_data(clf, my_dataset, features_list)#==============================================================================#==============================================================================# GaussianNB(priors=None)# Accuracy: 0.85629       Precision: 0.49612      Recall: 0.38400 F1: 0.43292     F2: 0.40218# Total predictions: 14000        True positives:  768    False positives:  780   False negatives: 1232   True negatives: 11220# #==============================================================================#==============================================================================